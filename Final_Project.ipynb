{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "_MIxqOW7YGH6",
        "outputId": "f3e8c7ea-08c6-4848-bba0-4ad092e70d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,499 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [966 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,569 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,288 kB]\n",
            "Fetched 14.6 MB in 4s (3,735 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=8894a32c3c0aa5c61b06bfad2f7847c760ca92bd537efce8a04488014f5fe52a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cwwpksa6Ztjb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GraphFrameExample\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "6gdAGQ3f3DxM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1yhFvd4Y45q"
      },
      "outputs": [],
      "source": [
        "#edges_df = spark.read.parquet(\"EdgesFinalExam/\")\n",
        "#edges_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irqtbQf_dJI4"
      },
      "outputs": [],
      "source": [
        "#vertics_df = spark.read.parquet(\"VertFinalExam/\")\n",
        "#vertics_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fzXxFZknd1vn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DD1yLiKwdV5R"
      },
      "outputs": [],
      "source": [
        "edges_schema = StructType([\n",
        "    StructField(\"src\", StringType(), True),\n",
        "    StructField(\"dst\", StringType(), True),\n",
        "    StructField(\"tripid\", IntegerType(), True),\n",
        "    StructField(\"delay\", IntegerType(), True),\n",
        "    StructField(\"distance\", IntegerType(), True)\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ij0kQKJ0eB7M"
      },
      "outputs": [],
      "source": [
        "vert_schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True),\n",
        "    StructField(\"Country\", StringType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xYL0O36HeVsk"
      },
      "outputs": [],
      "source": [
        "# Streaming DataFrame for Edges\n",
        "edges_df = spark.readStream \\\n",
        "    .schema(edges_schema) \\\n",
        "    .parquet(\"Edges_Data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HBwrsLJo7Dgk"
      },
      "outputs": [],
      "source": [
        "# Streaming DataFrame for Edges\n",
        "vertices_df = spark.readStream \\\n",
        "    .schema(vert_schema) \\\n",
        "    .parquet(\"Vertices_Data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA0_NonlWBmM",
        "outputId": "82665b0e-0fc6-4e1f-b618-ad399173008e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "edges_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uYAyjJ87PIY",
        "outputId": "f67730a7-1cde-489b-d05d-d8ddf0a98c03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vertices_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jER_pM1TfJKF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "edges_stream_df = edges_df.withColumn(\n",
        "    \"delay_category\",\n",
        "    when(col(\"delay\") < 0, \"Early\")\n",
        "    .when(col(\"delay\") > 0, \"Late\")\n",
        "    .otherwise(\"OnTime\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g-wszknVhCCO"
      },
      "outputs": [],
      "source": [
        "vertices_stream_df = vertices_df.filter(col(\"state\") != \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NoBxiAh2haA8"
      },
      "outputs": [],
      "source": [
        "edges_output_path = \"Edges_Sink/\"\n",
        "\n",
        "edges_writer = edges_stream_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", edges_output_path) \\\n",
        "    .option(\"checkpointLocation\", \"Edges_Checkpoint/\") \\\n",
        "    .outputMode(\"append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EA4sdunoAUGP"
      },
      "outputs": [],
      "source": [
        "vertices_output_path = \"Vertices_Sink/\"\n",
        "\n",
        "vertices_writer = vertices_stream_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", vertices_output_path) \\\n",
        "    .option(\"checkpointLocation\", \"Vertices_Checkpoint/\") \\\n",
        "    .outputMode(\"append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5RtYNW5zhrF8"
      },
      "outputs": [],
      "source": [
        "q1 = edges_writer.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1.stop()"
      ],
      "metadata": {
        "id": "oFreFc0X3WA7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2 = vertices_writer.start()"
      ],
      "metadata": {
        "id": "unsOzedOnSQs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2.stop()"
      ],
      "metadata": {
        "id": "_UvMWkMHnWTE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S11nJ3MMEfrn",
        "outputId": "ab4ca429-8eeb-402f-e07a-44e565e225fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+-----+--------+--------------+\n",
            "|src|dst| tripid|delay|distance|delay_category|\n",
            "+---+---+-------+-----+--------+--------------+\n",
            "|RSW|EWR|1010630|  -10|     928|         Early|\n",
            "|RSW|ORD|1021029|   87|     974|          Late|\n",
            "|RSW|EWR|1021346|    0|     928|        OnTime|\n",
            "|RSW|EWR|1021044|   18|     928|          Late|\n",
            "|RSW|IAH|1021730|   29|     748|          Late|\n",
            "|RSW|ORD|1020535|  605|     974|          Late|\n",
            "|RSW|ORD|1021820|   71|     974|          Late|\n",
            "|RSW|EWR|1021743|    0|     928|        OnTime|\n",
            "|RSW|EWR|1022017|    0|     928|        OnTime|\n",
            "|RSW|IAH|1020600|   -2|     748|         Early|\n",
            "|RSW|CLE|1021214|   29|     891|          Late|\n",
            "|RSW|EWR|1020630|   -5|     928|         Early|\n",
            "|RSW|ORD|1031029|   13|     974|          Late|\n",
            "|RSW|EWR|1031346|  279|     928|          Late|\n",
            "|RSW|IAH|1031740|   29|     748|          Late|\n",
            "|RSW|ORD|1030535|    0|     974|        OnTime|\n",
            "|RSW|ORD|1031808|   -3|     974|         Early|\n",
            "|RSW|DEN|1031516|   -2|    1396|         Early|\n",
            "|RSW|EWR|1032017|   14|     928|          Late|\n",
            "|RSW|CLE|1031214|   17|     891|          Late|\n",
            "+---+---+-------+-----+--------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "edg_df = spark.read.parquet(\"Edges_Sink/\")\n",
        "edg_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3esBV1UzEOs1",
        "outputId": "e6126ecd-c335-4221-91aa-2bceec004119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+-----+-------+\n",
            "| id|         City|State|Country|\n",
            "+---+-------------+-----+-------+\n",
            "|ABE|    Allentown|   PA|    USA|\n",
            "|ABI|      Abilene|   TX|    USA|\n",
            "|ABQ|  Albuquerque|   NM|    USA|\n",
            "|ABR|     Aberdeen|   SD|    USA|\n",
            "|ABY|       Albany|   GA|    USA|\n",
            "|ACK|    Nantucket|   MA|    USA|\n",
            "|ACT|         Waco|   TX|    USA|\n",
            "|ACV|       Eureka|   CA|    USA|\n",
            "|ACY|Atlantic City|   NJ|    USA|\n",
            "|ADQ|       Kodiak|   AK|    USA|\n",
            "|AEX|   Alexandria|   LA|    USA|\n",
            "|AGS|      Augusta|   GA|    USA|\n",
            "|AHN|       Athens|   GA|    USA|\n",
            "|AIA|     Alliance|   NE|    USA|\n",
            "|AKN|  King Salmon|   AK|    USA|\n",
            "|ALB|       Albany|   NY|    USA|\n",
            "|ALO|     Waterloo|   IA|    USA|\n",
            "|ALS|      Alamosa|   CO|    USA|\n",
            "|ALW|  Walla Walla|   WA|    USA|\n",
            "|AMA|     Amarillo|   TX|    USA|\n",
            "+---+-------------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vert_df = spark.read.parquet(\"Vertices_Sink/\")\n",
        "vert_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Spark version:\", spark.version)"
      ],
      "metadata": {
        "id": "BbxJPZHp1cpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphframes\n"
      ],
      "metadata": {
        "id": "34971COk2LED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e5cc94-b931-4915-a3dd-92d24af19fc6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes import GraphFrame"
      ],
      "metadata": {
        "id": "hmj38gpQVoda"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = GraphFrame(vert_df, edg_df)\n",
        "\n",
        "# Run PageRank algorithm\n",
        "results = g.pageRank(resetProbability=0.15, maxIter=5)\n",
        "\n",
        "# Get top 10 vertices by rank\n",
        "top_vertices = results.vertices.orderBy(\"pagerank\", ascending=False).limit(10)\n",
        "\n",
        "# Show the top vertices\n",
        "top_vertices.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfEPRBJkWeL6",
        "outputId": "75454aea-e786-46bd-9465-61315ae5ba78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-----+-------+------------------+\n",
            "| id|          City|State|Country|          pagerank|\n",
            "+---+--------------+-----+-------+------------------+\n",
            "|ATL|       Atlanta|   GA|    USA|31.402169285067313|\n",
            "|DFW|        Dallas|   TX|    USA| 22.76415219751248|\n",
            "|ORD|       Chicago|   IL|    USA| 21.83241348762772|\n",
            "|DEN|        Denver|   CO|    USA|16.026921025779515|\n",
            "|LAX|   Los Angeles|   CA|    USA|14.358865452635795|\n",
            "|IAH|       Houston|   TX|    USA|13.229634347806075|\n",
            "|SFO| San Francisco|   CA|    USA|11.322517232690489|\n",
            "|PHX|       Phoenix|   AZ|    USA|10.852423159730376|\n",
            "|SLC|Salt Lake City|   UT|    USA| 9.622759351860472|\n",
            "|LAS|     Las Vegas|   NV|    USA| 8.778471071473987|\n",
            "+---+--------------+-----+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}